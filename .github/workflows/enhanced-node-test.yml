# .github/workflows/enhanced-node-test.yml

# å¢å¼ºç‰ˆèŠ‚ç‚¹æµ‹è¯•å·¥ä½œæµï¼Œæ”¯æŒæµ‹é€Ÿå’Œæµåª’ä½“è§£é”æ£€æµ‹

name: ğŸš€ Enhanced Node Testing & Update

on:

# å®šæ—¶è¿è¡Œ - æ¯4å°æ—¶ï¼ˆæµ‹è¯•è¾ƒè€—æ—¶ï¼‰

schedule:
- cron: â€˜0 */4 * * *â€™  # æ¯4å°æ—¶è¿è¡Œä¸€æ¬¡
# - cron: â€˜0 6,14,22 * * *â€™  # æ¯å¤©6ç‚¹ã€14ç‚¹ã€22ç‚¹è¿è¡Œ

# æ‰‹åŠ¨è¿è¡Œ

workflow_dispatch:
inputs:
test_mode:
description: â€˜æµ‹è¯•æ¨¡å¼â€™
required: false
default: â€˜fullâ€™
type: choice
options:
- â€˜basicâ€™    # åŸºç¡€è¿é€šæ€§æµ‹è¯•
- â€˜speedâ€™    # åŒ…å«é€Ÿåº¦æµ‹è¯•
- â€˜fullâ€™     # å®Œæ•´æµ‹è¯•ï¼ˆåŒ…å«æµåª’ä½“ï¼‰
max_nodes:
description: â€˜æœ€å¤§æµ‹è¯•èŠ‚ç‚¹æ•°â€™
required: false
default: â€˜50â€™
type: string

env:
TZ: â€˜Asia/Shanghaiâ€™
PYTHONUNBUFFERED: â€˜1â€™

jobs:

# ç¬¬ä¸€ä¸ªä»»åŠ¡ï¼šè·å–å’Œè§£æèŠ‚ç‚¹

fetch-nodes:
runs-on: ubuntu-latest
outputs:
nodes-count: ${{ steps.parse.outputs.nodes-count }}

```
steps:
- name: ğŸ“¥ Checkout Repository
  uses: actions/checkout@v4
  with:
    token: ${{ secrets.GITHUB_TOKEN }}

- name: ğŸ Setup Python Environment
  uses: actions/setup-python@v4
  with:
    python-version: '3.11'
    cache: 'pip'

- name: ğŸ“¦ Install Dependencies
  run: |
    pip install --upgrade pip
    pip install requests PyYAML aiohttp asyncio

- name: ğŸ”„ Fetch and Parse Nodes
  id: parse
  env:
    SUBSCRIPTION_URLS: ${{ secrets.SUBSCRIPTION_URLS }}
  run: |
    echo "ğŸ“¡ å¼€å§‹è·å–è®¢é˜…èŠ‚ç‚¹..."
    python3 << 'EOF'
    import os
    import json
    import requests
    import base64
    from urllib.parse import urlparse, parse_qs, unquote
    import re
    import hashlib
    
    def fetch_subscription(url):
        """è·å–è®¢é˜…å†…å®¹"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            content = response.text.strip()
            try:
                decoded = base64.b64decode(content).decode('utf-8')
                content = decoded
            except:
                pass
            
            links = [line.strip() for line in content.split('\n') 
                    if line.strip() and any(line.startswith(p) for p in ['vmess://', 'vless://', 'trojan://', 'ss://'])]
            
            print(f"ä» {url[:50]}... è·å–åˆ° {len(links)} ä¸ªèŠ‚ç‚¹")
            return links
        except Exception as e:
            print(f"è·å–è®¢é˜…å¤±è´¥ {url}: {e}")
            return []
    
    def parse_vmess(url):
        """è§£ævmessèŠ‚ç‚¹"""
        try:
            data = json.loads(base64.b64decode(url[8:]).decode('utf-8'))
            return {
                'protocol': 'vmess',
                'name': data.get('ps', ''),
                'server': data.get('add', ''),
                'port': int(data.get('port', 443)),
                'uuid': data.get('id', ''),
                'method': data.get('scy', 'auto'),
                'network': data.get('net', 'tcp'),
                'path': data.get('path', ''),
                'host': data.get('host', ''),
                'tls': data.get('tls', ''),
                'raw_url': url
            }
        except:
            return None
    
    def parse_node(url):
        """è§£æèŠ‚ç‚¹é“¾æ¥"""
        if url.startswith('vmess://'):
            return parse_vmess(url)
        # å…¶ä»–åè®®è§£æ...
        return None
    
    def detect_country(server, name):
        """æ£€æµ‹å›½å®¶"""
        text = (server + ' ' + name).lower()
        countries = {
            '
```
